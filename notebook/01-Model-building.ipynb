{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Data visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Data statistics\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Disable python warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('data/gemstone.csv')\n",
    "# Top & Last 5 rows of data\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary feature\n",
    "df.drop(columns=['id'], axis= 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical columns\n",
    "num = [col for col in df.columns if (df[col].dtypes == 'int64') or (df[col].dtypes == 'float64')]\n",
    "num.pop() # Drop last dependent column\n",
    "print(f\"List of numerical columns:\\n {num}\")\n",
    "print(f'Length of numerical columns: {len(num)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical columns\n",
    "cat = df.select_dtypes(include='object').columns\n",
    "print(f\"List of categorical columns:\\n {cat}\")\n",
    "print(f'Length of categorical columns: {len(cat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I made a few strategies to deal with data.\n",
    "\n",
    "### 1. Data Preprocessing\n",
    "### 2. Model Building, Cross-Validation, and Evaluation\n",
    "### 3. Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical data into numerical form using onehotencoding technique\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Apply power transform featurewise to make data more Gaussian-like.\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Applies transformers to columns of an array or pandas DataFrame.\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical ColumnTransformer\n",
    "cat_trf = ColumnTransformer(\n",
    "    transformers= [\n",
    "        ('ordinal_encoder', OrdinalEncoder(dtype='int64'), cat),\n",
    "    ],\n",
    "    verbose_feature_names_out=False,\n",
    "    remainder='passthrough'\n",
    ").set_output(transform='pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical columns into numerical form\n",
    "trf = cat_trf.fit_transform(df[cat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop ['cut', 'color', 'clarity'] columns\n",
    "df.drop(columns= cat, axis=1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the columns\n",
    "df = pd.concat([df, trf], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Columns Plotting the distplots without any transformation\n",
    "for col in df[num].columns:\n",
    "    plt.figure(figsize=(14,4))\n",
    "    plt.subplot(121)\n",
    "    sns.distplot(df[col])\n",
    "    plt.title(col)\n",
    "\n",
    "    plt.subplot(122)\n",
    "    stats.probplot(df[col], dist=\"norm\", plot=plt)\n",
    "    plt.title(col)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Columns Plotting the distplots without any transformation\n",
    "for col in df[cat].columns:\n",
    "    plt.figure(figsize=(14,4))\n",
    "    plt.subplot(121)\n",
    "    sns.distplot(df[col])\n",
    "    plt.title(col)\n",
    "\n",
    "    plt.subplot(122)\n",
    "    stats.probplot(df[col], dist=\"norm\", plot=plt)\n",
    "    plt.title(col)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into featue X and y for feature selection\n",
    "X = df.drop(columns=['price'], axis=1)\n",
    "y = df[['price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply power transform featurewise to make data more Gaussian-like.\n",
    "# # Transform features using quantiles information.\n",
    "# from sklearn.preprocessing import PowerTransformer\n",
    "# qwt = PowerTransformer(method='box-cox')\n",
    "# qwt.set_output(transform='pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Transform independent features\n",
    "# x_qwt = qwt.fit_transform(X+0.00000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Before and after comparision\n",
    "# for col in x_qwt.columns:\n",
    "#     plt.figure(figsize=(14,4))\n",
    "#     plt.subplot(121)\n",
    "#     sns.distplot(X[col])\n",
    "#     plt.title(col)\n",
    "\n",
    "#     plt.subplot(122)\n",
    "#     sns.distplot(x_qwt[col])\n",
    "#     plt.title(col)\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_qwt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Feature selection (FS) using varianceThreshold & mutual_info_reg technique\n",
    "from sklearn.feature_selection import VarianceThreshold, mutual_info_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find zero variance features of x_qwt\n",
    "vt = VarianceThreshold(threshold= (0.95 * (1-0.95)))\n",
    "vt.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names you gave\n",
    "vt.feature_names_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names you get\n",
    "vt.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance values\n",
    "vt.variances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns \n",
    "vt.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi = mutual_info_regression(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = pd.Series(mi, X.columns[0:len(X.columns)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between the features\n",
    "df.corr()['price'].sort_values(ascending=False)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph heatmap of correlation\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.heatmap(df.corr(numeric_only=True), cmap='coolwarm', annot=True, annot_kws={'size': 12}, linewidths= .7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariance between the features\n",
    "cov = np.round(df.cov()['price'],4)\n",
    "cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Building, Cross-Validation, and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split arrays or matrices into random train and test subsets.\n",
    "# Evaluate a score by cross-validation.\n",
    "# Exhaustive search over specified parameter values for an estimator.\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "\n",
    "# Regressor model\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Different Regression Models\n",
    "elastinet = ElasticNet(alpha=0.3, l1_ratio=0.8, max_iter=800)\n",
    "decisiontreeregressor = DecisionTreeRegressor(criterion='absolute_error', max_depth=12, max_features='sqrt', \n",
    "                                              min_samples_leaf=8, min_samples_split=8)\n",
    "randomforestregressor = RandomForestRegressor(n_estimators= 32, criterion= 'poisson', max_features= 'sqrt', \n",
    "                                              max_depth= 6, min_samples_leaf= 3, min_samples_split= 3)\n",
    "gradientboostingregressor = GradientBoostingRegressor(n_estimators= 32, criterion= 'squared_error', \n",
    "                                                      max_features= 'sqrt', loss= 'squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Regression models into list\n",
    "models = [elastinet, decisiontreeregressor, randomforestregressor, gradientboostingregressor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the cross_val_score on all the models\n",
    "def cross_src(model,X_trn,y_trn):\n",
    "    src = np.round(cross_val_score(model, X_trn, y_trn, cv=5),2)\n",
    "    msrc = np.round(np.mean(src),2)\n",
    "    return src, msrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with cross_val_score\n",
    "result = []\n",
    "mean_src = []\n",
    "\n",
    "for model in models:\n",
    "    cur_rst,cur_msrc = cross_src(model, X_train, y_train)\n",
    "    print('Model Name: ',model)\n",
    "    print('\\n')\n",
    "    print('Result :', cur_rst)\n",
    "    print('Mean_Score :', cur_msrc)\n",
    "    print('\\n')\n",
    "    \n",
    "    result.append(cur_rst)\n",
    "    mean_src.append(cur_msrc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform RandomizedSearchCV with cross-validation to find the best hyperparameter\n",
    "params = {\n",
    "    'elasticnet': {\n",
    "        # 'alpha': [0.3, 0.6, 0.8, 1.0],\n",
    "        # 'l1_ratio': [0.3, 0.6, 0.8, 1.0],\n",
    "        # 'max_iter': [250, 500, 800, 1000],\n",
    "        # 'selection': ['cyclic', 'random'],\n",
    "    },\n",
    "    'decisiontreeregressor': {\n",
    "        # 'criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "        # 'splitter':['best','random'],\n",
    "        # 'max_features':['sqrt','log2', 'auto'],\n",
    "        # 'max_depth': [4, 6, 8],\n",
    "        # 'min_samples_split': [2, 3, 4],\n",
    "        # 'min_samples_leaf': [1, 2, 3, 4],\n",
    "    },\n",
    "    'randomforestregressor': {\n",
    "        # 'n_estimators': [8, 16, 32],\n",
    "        # 'criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "        # 'max_depth': [4, 6, 8],\n",
    "        # 'min_samples_split': [2, 3, 4],\n",
    "        # 'min_samples_leaf': [1, 2, 3, 4],\n",
    "    },\n",
    "    'gradientboostingregressor': {\n",
    "        # 'loss':['squared_error', 'huber', 'absolute_error', 'quantile'],\n",
    "        # 'learning_rate':[.1,.01,.05,.001],\n",
    "        # 'subsample':[0.6,0.7,0.75,0.8,0.85,0.9],\n",
    "        # 'criterion':['squared_error', 'friedman_mse'],\n",
    "        # 'max_features':['auto','sqrt','log2'],\n",
    "        # 'n_estimators': [8, 16, 32]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the train or test all loss/metrics the models \n",
    "def evaluate_model(true, prediction):\n",
    "    mae = np.round(mean_absolute_error(true, prediction), 2)\n",
    "    mape = np.round(mean_absolute_percentage_error(true, prediction), 2)\n",
    "    r2 = np.round(r2_score(true, prediction), 2)\n",
    "    ad_r2 = np.round(1-((1 - r2)*(38715 - 1) / (38715 - 23 - 1)), 2)\n",
    "    \n",
    "    return mae, mape, r2, ad_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models and get the traning loss/metrics evaluation\n",
    "mae_src= []\n",
    "mape_src= []\n",
    "r2_src= []\n",
    "adjt_r2 = []\n",
    "train_src = []\n",
    "test_src = []\n",
    "results = []\n",
    "\n",
    "for model in models:\n",
    "    param = params[model.__class__.__name__.lower()]\n",
    "\n",
    "    gs = RandomizedSearchCV(model, param_distributions=param, cv=3, verbose=1)\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    model.set_params(**gs.best_params_)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make prediction\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate model on test dataset\n",
    "    trn_mae, trn_mape, trn_r2, trn_adj_r2 = evaluate_model(y_train, y_train_pred)\n",
    "    cur_mae, cur_mape, cur_r2, cur_adj_r2 = evaluate_model(y_test, y_test_pred)\n",
    "    \n",
    "    print('Model performance on Test dataset')\n",
    "    print('Model Name: ',model)\n",
    "    print('\\n')\n",
    "    print('Mean_Absolute_Error :',cur_mae)\n",
    "    print('Mean_Absolute_Precentage_Error :',cur_mape)\n",
    "    print('R2_Score :',cur_r2)\n",
    "    print('Adjusted_R2_Score :',cur_adj_r2)\n",
    "    print('-'*30)\n",
    "    print('\\n')\n",
    "\n",
    "    print('Model performance on Training dataset')\n",
    "    print(f\"Mean Absolute Error: {trn_mae}\")\n",
    "    print(f'Mean Absolute Precentage Error : {trn_mape}')\n",
    "    print(f\"R2 Score: {trn_r2}\")\n",
    "    print(f'Adjusted R2 Score : {trn_adj_r2}')\n",
    "    print('-'*35)\n",
    "    print('\\n')\n",
    "\n",
    "    mae_src.append(cur_mae)\n",
    "    mape_src.append(cur_mape)\n",
    "    r2_src.append(cur_r2)\n",
    "    adjt_r2.append(cur_r2)\n",
    "    \n",
    "    results.append({\n",
    "        'model': model.__class__.__name__,\n",
    "        'y_actual': y_test,\n",
    "        'y_pred': y_test_pred,\n",
    "        'mae': cur_mae,\n",
    "        'mape': cur_mape,\n",
    "        'r2': cur_r2,\n",
    "        'adj_r2': cur_adj_r2\n",
    "    })\n",
    "    \n",
    "    print('f*** Model {model} Detail ***')\n",
    "    train = np.round(model.score(X_train, y_train), 2)\n",
    "    test = np.round(model.score(X_test, y_test), 2)\n",
    "    print(f\" Training Model score :\\n {train}\")\n",
    "    print(f\" Testing Model score :\\n {test}\")\n",
    "    print('='*30)\n",
    "    train_src.append(train)\n",
    "    test_src.append(test)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the metrics graph of MAE & MAPE\n",
    "epochs = range(len(models))\n",
    "fig, ax = plt.subplots(figsize=(15,6))\n",
    "plt.plot(epochs, mae_src, '-o', color='blue', label='MAE')\n",
    "plt.plot(epochs, mape_src, '-o', color='green', label='MAPE')\n",
    "\n",
    "ax.set_xticklabels(range(0,11,1), rotation = 90)\n",
    "ax.set_xlabel(xlabel= models)\n",
    "plt.grid(visible=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the metrics graph of R2 & Adjusted R2\n",
    "epochs = range(len(models))\n",
    "fig, ax = plt.subplots(figsize=(15,6))\n",
    "plt.plot(epochs, r2_src, '-o', color='red', label='R2')\n",
    "plt.plot(epochs, adjt_r2, '-o', color='black', label='Adjusted R2')\n",
    "\n",
    "ax.set_xticklabels(range(0,11,1), rotation = 90)\n",
    "ax.set_xlabel(xlabel= models)\n",
    "plt.grid(visible=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the metrics graph\n",
    "epochs = range(1, len(models) +1 )\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(epochs, mae_src, marker='o', linestyle='-', color='blue', label='MAE')\n",
    "plt.plot(epochs, mape_src, marker='o', linestyle='-', color='green', label='MAPE')\n",
    "plt.plot(epochs, r2_src, marker='o', linestyle='-', color='yellow', label='R2')\n",
    "plt.plot(epochs, adjt_r2, marker='o', linestyle='-', color='purple', label='Adjusted R2')\n",
    "\n",
    "plt.title('Comparison of Metrics over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Metric Values')\n",
    "plt.grid(visible=True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All models error scores are convert into dataframe\n",
    "perform_df = pd.DataFrame({'Algorithm': models, 'MAE': mae_src, 'MAPE': mape_src, 'R2': r2_src, 'Adjusted R2': adjt_r2, 'Model Train Score': train_src, 'Model Test Score': test_src})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting actual vs. predicted values for each model\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for result in results:\n",
    "    plt.scatter(result['y_actual'], result['y_pred'], label=result['model'], alpha=0.7)\n",
    "\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red', label='Perfect Prediction')\n",
    "\n",
    "plt.title('Actual vs. Predicted Values for Different Models')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.loc[48344]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.loc[48344]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[48344]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Users Input\n",
    "user_input = np.array([0.55, 60.60, 59.00, 5.28, 5.31, 3.21, 3.00, 2.00, 2.00]).reshape(1, -1)\n",
    "user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    # trf = qwt.transform(user_input)\n",
    "    pred = model.predict(user_input)[0]\n",
    "\n",
    "    print('Model Name: ',model)\n",
    "    print('Predict price :', np.round(pred,2))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
